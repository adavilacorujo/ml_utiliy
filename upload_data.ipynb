{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:04<00:00,  2.38s/it]\n",
      "100%|██████████| 2/2 [00:04<00:00,  2.36s/it]\n",
      "100%|██████████| 2/2 [00:04<00:00,  2.42s/it]\n",
      "100%|██████████| 2/2 [00:04<00:00,  2.37s/it]\n",
      "100%|██████████| 2/2 [00:04<00:00,  2.36s/it]\n",
      "100%|██████████| 2/2 [00:04<00:00,  2.37s/it]\n",
      "100%|██████████| 2/2 [00:04<00:00,  2.36s/it]\n",
      "100%|██████████| 2/2 [00:04<00:00,  2.35s/it]\n",
      "100%|██████████| 2/2 [00:04<00:00,  2.34s/it]\n",
      "100%|██████████| 2/2 [00:04<00:00,  2.34s/it]\n"
     ]
    }
   ],
   "source": [
    "#--------------------------------------------------------------------------\n",
    "# Copyright (c) 2022 USAF ACC/A29 Intel Data/Tech\n",
    "# Futures Division\n",
    "# All Rights Reserved.\n",
    "#\n",
    "# Dissemination of this information and/or reproduction\n",
    "# and modification are restricted to other  government\n",
    "# organizations.  Commercial use is strictly forbidden\n",
    "# unless prior written permission is obtained from ACC/A29\n",
    "# Intel Data/Tech Futures Division.\n",
    "#\n",
    "#--------------------------------------------------------------------------\n",
    "__author__ = 'Andres Davila'\n",
    "__version__ = '0.1'\n",
    "__email__ = 'andres.davila-corujo@us.af.mil'\n",
    "\n",
    "import copy\n",
    "import uuid\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from elasticsearch7 import Elasticsearch, helpers\n",
    "\n",
    "\n",
    "data = pd.DataFrame()\n",
    "es = Elasticsearch([\"http://localhost:9200\"],\n",
    "                   http_auth=('elastic', 'p3DzCcQvxt5Mg8TaEE61'), timeout=6000)\n",
    "\n",
    "\n",
    "def read_data(file_path : str, delimiter : str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read data from csv file\n",
    "    \"\"\"\n",
    "\n",
    "    data = pd.read_csv(file_path, delimiter=delimiter)\n",
    "\n",
    "    return data\n",
    "\n",
    "def get_indices(es):\n",
    "    return [index for index in list(es.indices.get_alias(\"*\").keys()) if not index.startswith('.')]\n",
    "\n",
    "def get_index_features(es, index_name):\n",
    "    \"\"\"Get features in an index\n",
    "\n",
    "    Args:\n",
    "        es (Elasticsearch), Elasticsearch object\n",
    "        index_name (str), index name\n",
    "\n",
    "    Return:\n",
    "        a list of str\n",
    "    \"\"\"\n",
    "    index_mapping = es.indices.get_mapping(index = index_name)\n",
    "    features = index_mapping[index_name]['mappings']['properties']\n",
    "    features_copy = copy.deepcopy(features)\n",
    "\n",
    "    for feature in features:\n",
    "        if 'properties' in features[feature]:\n",
    "            for key in features[feature]['properties']:\n",
    "                features_copy[feature+'.'+key] =  features[feature]['properties'][key]\n",
    "            del features_copy[feature]\n",
    "\n",
    "    return list(features_copy.keys())\n",
    "\n",
    "\n",
    "\n",
    "def filter_keys(document, common): \n",
    "    return {key: document[key] for key in common}\n",
    "\n",
    "\n",
    "def doc_generator(df, index, common):\n",
    "    df_iter = df.iterrows()\n",
    "    for _, document in df_iter:\n",
    "        yield {\n",
    "            '_index' : index,\n",
    "            '_type'  : '_doc',\n",
    "            '_id'    : f\"{uuid.uuid4()}\",\n",
    "            \"_source\": filter_keys(document, common)\n",
    "        }\n",
    "\n",
    "    raise StopIteration\n",
    "\n",
    "\n",
    "def add_data(hour, file_path, index_name):\n",
    "\n",
    "    data = read_data(file_path, '\\t')\n",
    "\n",
    "    data['@timestamp'] = data['@timestamp'].apply(lambda x: (datetime.now() - timedelta(hours=hour)).strftime(\"%Y-%m-%dT%H:%M:%S.000Z\"))\n",
    "\n",
    "    # index_name = get_indices(es)\n",
    "\n",
    "    # if len(index_name) < 0:\n",
    "    #     # create index\n",
    "    #     index_name = 'filebeat'\n",
    "\n",
    "\n",
    "    feature_names = data.columns.to_list()\n",
    "\n",
    "    actions = list()\n",
    "    data = data.fillna('')\n",
    "    # data = data[:1]\n",
    "    index = 0\n",
    "    while True:\n",
    "        line = data.iloc[index].to_dict()\n",
    "        # record = json.dumps(line)\n",
    "        record = line\n",
    "        record_ingest = {}\n",
    "\n",
    "        for features_name in feature_names:\n",
    "            if features_name not in record:\n",
    "                record_ingest[features_name] = None\n",
    "            else:\n",
    "                record_ingest[features_name] = record[str(features_name)]\n",
    "\n",
    "        action = {'_index' : index_name, '_source': record_ingest}\n",
    "        actions.append(action)\n",
    "\n",
    "        index += 1\n",
    "\n",
    "        if(index%1000 == 0 and index > 0):\n",
    "            helpers.bulk(es, actions)\n",
    "            actions = list()\n",
    "\n",
    "\n",
    "        if index == len(data):\n",
    "            break\n",
    "\n",
    "    if len(actions) > 0:\n",
    "        helpers.bulk(es, actions)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    for _ in tqdm.tqdm(range(10)):\n",
    "        hours = range(-1, 1)\n",
    "\n",
    "        for hour in hours:\n",
    "            add_data(hour, file_path='./data/zeek_AD.csv', index_name='zeek_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/45/pmj5rh4s5v35wphtcbbmymbm0000gn/T/ipykernel_29013/2454231526.py:20: DeprecationWarning: The 'body' parameter is deprecated for the 'index' API and will be removed in a future version. Instead use the 'document' parameter. See https://github.com/elastic/elasticsearch-py/issues/1698 for more information\n",
      "  es.index(index=index, doc_type='Blog', id=i, body=line)\n",
      "/opt/homebrew/anaconda3/envs/perp/lib/python3.11/site-packages/elasticsearch7/connection/base.py:200: ElasticsearchWarning: [types removal] Specifying types in document index requests is deprecated, use the typeless endpoints instead (/{index}/_doc/{id}, /{index}/_doc, or /{index}/_create/{id}).\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "from pprint import pprint\n",
    "from elasticsearch7 import Elasticsearch, helpers\n",
    "\n",
    "\n",
    "data = pd.DataFrame()\n",
    "es = Elasticsearch([\"http://localhost:9200\"],\n",
    "                   http_auth=('elastic', 'p3DzCcQvxt5Mg8TaEE61'), timeout=6000)\n",
    "\n",
    "MyFile= open(\"./data/dns.log\",'r').read()\n",
    "ClearData = MyFile.splitlines(True)\n",
    "i=0\n",
    "json_str=\"\"\n",
    "docs ={}\n",
    "index = 'dns_test'\n",
    "\n",
    "for line in ClearData:\n",
    "    line = ''.join(line.split())\n",
    "    es.index(index=index, doc_type='Blog', id=i, body=line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "perp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
